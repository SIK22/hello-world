{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"try2.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMMr8h4T1LoozcVbXAO3Gg/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"49c56e07efa04527914930e650f7e4c9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_b800b9a9d6fb4db2ae18389698b1584e","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_e028ddb0c8bb43669dec22485da6cb9c","IPY_MODEL_918437bf3f1a4d18ab30e6b7afe6aaed"]}},"b800b9a9d6fb4db2ae18389698b1584e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e028ddb0c8bb43669dec22485da6cb9c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_26bb29e1d30347ad899855a31f70984d","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":21388428,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":21388428,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5919f70635494c13ad3f82c3e959d123"}},"918437bf3f1a4d18ab30e6b7afe6aaed":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_02393bf35006414e9ea293550b398614","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 20.4M/20.4M [00:02&lt;00:00, 9.34MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d812ca3ef33f4175b8ea4964749c70bb"}},"26bb29e1d30347ad899855a31f70984d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"5919f70635494c13ad3f82c3e959d123":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"02393bf35006414e9ea293550b398614":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"d812ca3ef33f4175b8ea4964749c70bb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G1lhpT4Xoehd","executionInfo":{"status":"ok","timestamp":1615270725129,"user_tz":-540,"elapsed":20278,"user":{"displayName":"Eunah choi","photoUrl":"","userId":"01586166681599241417"}},"outputId":"07d53314-52f5-4f78-85cb-cc678babb7da"},"source":["from google.colab import drive\r\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0M1d147aovZk","executionInfo":{"status":"ok","timestamp":1615270792244,"user_tz":-540,"elapsed":9798,"user":{"displayName":"Eunah choi","photoUrl":"","userId":"01586166681599241417"}},"outputId":"b1c7b147-be10-41c4-96f1-cdf7e8069614"},"source":["!pip install -U efficientnet\r\n","!pip install -U git+https://github.com/qubvel/efficientnet\r\n","!pip install -U --pre efficientnet"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting efficientnet\n","  Downloading https://files.pythonhosted.org/packages/53/97/84f88e581d6ac86dcf1ab347c497c4c568c38784e3a2bd659b96912ab793/efficientnet-1.1.1-py3-none-any.whl\n","Collecting keras-applications<=1.0.8,>=1.0.7\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n","\r\u001b[K     |██████▌                         | 10kB 28.9MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 20kB 35.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 30kB 39.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 40kB 41.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 8.1MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: scikit-image in /usr/local/lib/python3.7/dist-packages (from efficientnet) (0.16.2)\n","Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications<=1.0.8,>=1.0.7->efficientnet) (2.10.0)\n","Requirement already satisfied, skipping upgrade: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras-applications<=1.0.8,>=1.0.7->efficientnet) (1.19.5)\n","Requirement already satisfied, skipping upgrade: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->efficientnet) (1.1.1)\n","Requirement already satisfied, skipping upgrade: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->efficientnet) (3.2.2)\n","Requirement already satisfied, skipping upgrade: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->efficientnet) (2.4.1)\n","Requirement already satisfied, skipping upgrade: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->efficientnet) (2.5)\n","Requirement already satisfied, skipping upgrade: pillow>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->efficientnet) (7.0.0)\n","Requirement already satisfied, skipping upgrade: scipy>=0.19.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->efficientnet) (1.4.1)\n","Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications<=1.0.8,>=1.0.7->efficientnet) (1.15.0)\n","Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet) (0.10.0)\n","Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet) (2.4.7)\n","Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet) (2.8.1)\n","Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet) (1.3.1)\n","Requirement already satisfied, skipping upgrade: decorator>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.0->scikit-image->efficientnet) (4.4.2)\n","Installing collected packages: keras-applications, efficientnet\n","Successfully installed efficientnet-1.1.1 keras-applications-1.0.8\n","Collecting git+https://github.com/qubvel/efficientnet\n","  Cloning https://github.com/qubvel/efficientnet to /tmp/pip-req-build-r4sooehy\n","  Running command git clone -q https://github.com/qubvel/efficientnet /tmp/pip-req-build-r4sooehy\n","Requirement already satisfied, skipping upgrade: keras_applications<=1.0.8,>=1.0.7 in /usr/local/lib/python3.7/dist-packages (from efficientnet==1.1.1) (1.0.8)\n","Requirement already satisfied, skipping upgrade: scikit-image in /usr/local/lib/python3.7/dist-packages (from efficientnet==1.1.1) (0.16.2)\n","Requirement already satisfied, skipping upgrade: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras_applications<=1.0.8,>=1.0.7->efficientnet==1.1.1) (1.19.5)\n","Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.7/dist-packages (from keras_applications<=1.0.8,>=1.0.7->efficientnet==1.1.1) (2.10.0)\n","Requirement already satisfied, skipping upgrade: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->efficientnet==1.1.1) (2.4.1)\n","Requirement already satisfied, skipping upgrade: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->efficientnet==1.1.1) (3.2.2)\n","Requirement already satisfied, skipping upgrade: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->efficientnet==1.1.1) (2.5)\n","Requirement already satisfied, skipping upgrade: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->efficientnet==1.1.1) (1.1.1)\n","Requirement already satisfied, skipping upgrade: scipy>=0.19.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->efficientnet==1.1.1) (1.4.1)\n","Requirement already satisfied, skipping upgrade: pillow>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->efficientnet==1.1.1) (7.0.0)\n","Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from h5py->keras_applications<=1.0.8,>=1.0.7->efficientnet==1.1.1) (1.15.0)\n","Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.1.1) (2.8.1)\n","Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.1.1) (1.3.1)\n","Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.1.1) (0.10.0)\n","Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.1.1) (2.4.7)\n","Requirement already satisfied, skipping upgrade: decorator>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.0->scikit-image->efficientnet==1.1.1) (4.4.2)\n","Building wheels for collected packages: efficientnet\n","  Building wheel for efficientnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for efficientnet: filename=efficientnet-1.1.1-cp37-none-any.whl size=18421 sha256=ade4794a0b1dba0bf48265368878658c94d6a0c5ad683d6665a2fb92c5d0c26a\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-up8w92b5/wheels/64/60/2e/30ebaa76ed1626e86bfb0cc0579b737fdb7d9ff8cb9522663a\n","Successfully built efficientnet\n","Installing collected packages: efficientnet\n","  Found existing installation: efficientnet 1.1.1\n","    Uninstalling efficientnet-1.1.1:\n","      Successfully uninstalled efficientnet-1.1.1\n","Successfully installed efficientnet-1.1.1\n","Requirement already up-to-date: efficientnet in /usr/local/lib/python3.7/dist-packages (1.1.1)\n","Requirement already satisfied, skipping upgrade: scikit-image in /usr/local/lib/python3.7/dist-packages (from efficientnet) (0.16.2)\n","Requirement already satisfied, skipping upgrade: keras-applications<=1.0.8,>=1.0.7 in /usr/local/lib/python3.7/dist-packages (from efficientnet) (1.0.8)\n","Requirement already satisfied, skipping upgrade: scipy>=0.19.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->efficientnet) (1.4.1)\n","Requirement already satisfied, skipping upgrade: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->efficientnet) (1.1.1)\n","Requirement already satisfied, skipping upgrade: pillow>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->efficientnet) (7.0.0)\n","Requirement already satisfied, skipping upgrade: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->efficientnet) (2.4.1)\n","Requirement already satisfied, skipping upgrade: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->efficientnet) (2.5)\n","Requirement already satisfied, skipping upgrade: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->efficientnet) (3.2.2)\n","Requirement already satisfied, skipping upgrade: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras-applications<=1.0.8,>=1.0.7->efficientnet) (1.19.5)\n","Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications<=1.0.8,>=1.0.7->efficientnet) (2.10.0)\n","Requirement already satisfied, skipping upgrade: decorator>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.0->scikit-image->efficientnet) (4.4.2)\n","Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet) (1.3.1)\n","Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet) (2.8.1)\n","Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet) (2.4.7)\n","Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet) (0.10.0)\n","Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications<=1.0.8,>=1.0.7->efficientnet) (1.15.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jez_w7GZqAVm","executionInfo":{"status":"ok","timestamp":1615271043907,"user_tz":-540,"elapsed":3715,"user":{"displayName":"Eunah choi","photoUrl":"","userId":"01586166681599241417"}},"outputId":"a2bd6aec-f3e9-464b-d1d0-361bda921b39"},"source":["pip install efficientnet_pytorch"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Collecting efficientnet_pytorch\n","  Downloading https://files.pythonhosted.org/packages/4e/83/f9c5f44060f996279e474185ebcbd8dbd91179593bffb9abe3afa55d085b/efficientnet_pytorch-0.7.0.tar.gz\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from efficientnet_pytorch) (1.8.0+cu101)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->efficientnet_pytorch) (3.7.4.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch->efficientnet_pytorch) (1.19.5)\n","Building wheels for collected packages: efficientnet-pytorch\n","  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.0-cp37-none-any.whl size=16031 sha256=7aa3dce184ad917dba88669bcf31cc5a60efe5d7b0b319b6a8edfce85ed23a73\n","  Stored in directory: /root/.cache/pip/wheels/e9/c6/e1/7a808b26406239712cfce4b5ceeb67d9513ae32aa4b31445c6\n","Successfully built efficientnet-pytorch\n","Installing collected packages: efficientnet-pytorch\n","Successfully installed efficientnet-pytorch-0.7.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jzV7YmlmovyR","executionInfo":{"status":"ok","timestamp":1615271052554,"user_tz":-540,"elapsed":4266,"user":{"displayName":"Eunah choi","photoUrl":"","userId":"01586166681599241417"}}},"source":["from efficientnet_pytorch import EfficientNet\r\n","model = EfficientNet.from_name('efficientnet-b0')"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":122,"referenced_widgets":["49c56e07efa04527914930e650f7e4c9","b800b9a9d6fb4db2ae18389698b1584e","e028ddb0c8bb43669dec22485da6cb9c","918437bf3f1a4d18ab30e6b7afe6aaed","26bb29e1d30347ad899855a31f70984d","5919f70635494c13ad3f82c3e959d123","02393bf35006414e9ea293550b398614","d812ca3ef33f4175b8ea4964749c70bb"]},"id":"XIhr0MBipFcM","executionInfo":{"status":"ok","timestamp":1615271056367,"user_tz":-540,"elapsed":1112,"user":{"displayName":"Eunah choi","photoUrl":"","userId":"01586166681599241417"}},"outputId":"97425950-6d17-46d1-b3a5-b2a5a91acdac"},"source":["from efficientnet_pytorch import EfficientNet\r\n","model = EfficientNet.from_pretrained('efficientnet-b0')"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Downloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b0-355c32eb.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet-b0-355c32eb.pth\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"49c56e07efa04527914930e650f7e4c9","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=21388428.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","Loaded pretrained weights for efficientnet-b0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":257},"id":"RbFtCMjApFhU","executionInfo":{"status":"error","timestamp":1615271058336,"user_tz":-540,"elapsed":609,"user":{"displayName":"Eunah choi","photoUrl":"","userId":"01586166681599241417"}},"outputId":"49f1d858-7e09-4d7f-e5e0-ee3458405e01"},"source":["from efficientnet_pytorch import EfficientNet\r\n","model = EfficientNet.from_pretrained('efficientnet-b0')\r\n","\r\n","# ... image preprocessing as in the classification example ...\r\n","print(img.shape) # torch.Size([1, 3, 224, 224])\r\n","\r\n","features = model.extract_features(img)\r\n","print(features.shape) # torch.Size([1, 1280, 7, 7])"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Loaded pretrained weights for efficientnet-b0\n"],"name":"stdout"},{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-1665fc363891>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# ... image preprocessing as in the classification example ...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# torch.Size([1, 3, 224, 224])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'img' is not defined"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dCoAYY32pFlb","executionInfo":{"status":"ok","timestamp":1615271102551,"user_tz":-540,"elapsed":738,"user":{"displayName":"Eunah choi","photoUrl":"","userId":"01586166681599241417"}},"outputId":"7f4264c6-bd54-4009-e316-7d285baa88f8"},"source":["import numpy as np\r\n","import json\r\n","from PIL import Image\r\n","import torch\r\n","import torch.nn as nn\r\n","import torch.optim as optim\r\n","from torch.optim import lr_scheduler\r\n","from torchvision import transforms\r\n","import matplotlib.pyplot as plt\r\n","import time\r\n","import os\r\n","import copy\r\n","import random\r\n","\r\n","from efficientnet_pytorch import EfficientNet\r\n","model_name = 'efficientnet-b0'  # b5\r\n","\r\n","image_size = EfficientNet.get_image_size(model_name)\r\n","print(image_size)\r\n","model = EfficientNet.from_pretrained(model_name, num_classes=12)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["224\n","Loaded pretrained weights for efficientnet-b0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TgZRTkPkpbJm","executionInfo":{"status":"ok","timestamp":1615271129794,"user_tz":-540,"elapsed":7131,"user":{"displayName":"Eunah choi","photoUrl":"","userId":"01586166681599241417"}},"outputId":"ac0b2a70-1d4b-44d9-e267-87dae3885f01"},"source":["batch_size  = 128\r\n","random_seed = 555\r\n","random.seed(random_seed)\r\n","torch.manual_seed(random_seed)\r\n","\r\n","## make dataset\r\n","from torchvision import transforms, datasets\r\n","data_path = './drive/MyDrive/mushroom_project/new_mush/'  # class 별 폴더로 나누어진걸 확 가져와서 라벨도 달아준다\r\n","mush_dataset = datasets.ImageFolder(\r\n","                                data_path,\r\n","                                transforms.Compose([\r\n","                                    transforms.Resize((224, 224)),\r\n","                                    transforms.ToTensor(),\r\n","                                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\r\n","                                ]))\r\n","## data split\r\n","from sklearn.model_selection import train_test_split\r\n","from torch.utils.data import Subset\r\n","train_idx, tmp_idx = train_test_split(list(range(len(mush_dataset))), test_size=0.2, random_state=random_seed)\r\n","datasets = {}\r\n","datasets['train'] = Subset(mush_dataset, train_idx)\r\n","tmp_dataset       = Subset(mush_dataset, tmp_idx)\r\n","\r\n","val_idx, test_idx = train_test_split(list(range(len(tmp_dataset))), test_size=0.5, random_state=random_seed)\r\n","datasets['valid'] = Subset(tmp_dataset, val_idx)\r\n","datasets['test']  = Subset(tmp_dataset, test_idx)\r\n","\r\n","## data loader 선언\r\n","dataloaders, batch_num = {}, {}\r\n","dataloaders['train'] = torch.utils.data.DataLoader(datasets['train'],\r\n","                                              batch_size=batch_size, shuffle=True,\r\n","                                              num_workers=4)\r\n","dataloaders['valid'] = torch.utils.data.DataLoader(datasets['valid'],\r\n","                                              batch_size=batch_size, shuffle=False,\r\n","                                              num_workers=4)\r\n","dataloaders['test']  = torch.utils.data.DataLoader(datasets['test'],\r\n","                                              batch_size=batch_size, shuffle=False,\r\n","                                              num_workers=4)\r\n","batch_num['train'], batch_num['valid'], batch_num['test'] = len(dataloaders['train']), len(dataloaders['valid']), len(dataloaders['test'])\r\n","print('batch_size : %d,  tvt : %d / %d / %d' % (batch_size, batch_num['train'], batch_num['valid'], batch_num['test']))"],"execution_count":9,"outputs":[{"output_type":"stream","text":["batch_size : 128,  tvt : 15 / 2 / 2\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"-HOg8i2Hp2pm","executionInfo":{"status":"ok","timestamp":1615271229570,"user_tz":-540,"elapsed":706,"user":{"displayName":"Eunah choi","photoUrl":"","userId":"01586166681599241417"}}},"source":["def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\r\n","    since = time.time()\r\n","\r\n","    best_model_wts = copy.deepcopy(model.state_dict())\r\n","    best_acc = 0.0\r\n","    train_loss, train_acc, valid_loss, valid_acc = [], [], [], []\r\n","    \r\n","    for epoch in range(num_epochs):\r\n","        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\r\n","        print('-' * 10)\r\n","\r\n","        # Each epoch has a training and validation phase\r\n","        for phase in ['train', 'valid']:\r\n","            if phase == 'train':\r\n","                model.train()  # Set model to training mode\r\n","            else:\r\n","                model.eval()   # Set model to evaluate mode\r\n","\r\n","            running_loss, running_corrects, num_cnt = 0.0, 0, 0\r\n","            \r\n","            # Iterate over data.\r\n","            for inputs, labels in dataloaders[phase]:\r\n","                inputs = inputs.to(device)\r\n","                labels = labels.to(device)\r\n","\r\n","                # zero the parameter gradients\r\n","                optimizer.zero_grad()\r\n","\r\n","                # forward\r\n","                # track history if only in train\r\n","                with torch.set_grad_enabled(phase == 'train'):\r\n","                    outputs = model(inputs)\r\n","                    _, preds = torch.max(outputs, 1)\r\n","                    loss = criterion(outputs, labels)\r\n","\r\n","                    # backward + optimize only if in training phase\r\n","                    if phase == 'train':\r\n","                        loss.backward()\r\n","                        optimizer.step()\r\n","\r\n","                # statistics\r\n","                running_loss += loss.item() * inputs.size(0)\r\n","                running_corrects += torch.sum(preds == labels.data)\r\n","                num_cnt += len(labels)\r\n","            if phase == 'train':\r\n","                scheduler.step()\r\n","            \r\n","            epoch_loss = float(running_loss / num_cnt)\r\n","            epoch_acc  = float((running_corrects.double() / num_cnt).cpu()*100)\r\n","            \r\n","            if phase == 'train':\r\n","                train_loss.append(epoch_loss)\r\n","                train_acc.append(epoch_acc)\r\n","            else:\r\n","                valid_loss.append(epoch_loss)\r\n","                valid_acc.append(epoch_acc)\r\n","            print('{} Loss: {:.2f} Acc: {:.1f}'.format(phase, epoch_loss, epoch_acc))\r\n","           \r\n","            # deep copy the model\r\n","            if phase == 'valid' and epoch_acc > best_acc:\r\n","                best_idx = epoch\r\n","                best_acc = epoch_acc\r\n","                best_model_wts = copy.deepcopy(model.state_dict())\r\n","#                 best_model_wts = copy.deepcopy(model.module.state_dict())\r\n","                print('==> best model saved - %d / %.1f'%(best_idx, best_acc))\r\n","\r\n","    time_elapsed = time.time() - since\r\n","    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\r\n","    print('Best valid Acc: %d - %.1f' %(best_idx, best_acc))\r\n","\r\n","    # load best model weights\r\n","    model.load_state_dict(best_model_wts)\r\n","    torch.save(model.state_dict(), 'president_model.pt')\r\n","    print('model saved')\r\n","    return model, best_idx, best_acc, train_loss, train_acc, valid_loss, valid_acc"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"h1s__vHTqvV9","executionInfo":{"status":"ok","timestamp":1615271282655,"user_tz":-540,"elapsed":709,"user":{"displayName":"Eunah choi","photoUrl":"","userId":"01586166681599241417"}}},"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")  # set gpu\r\n","\r\n","model = model.to(device)\r\n","\r\n","criterion = nn.CrossEntropyLoss()\r\n","\r\n","optimizer_ft = optim.SGD(model.parameters(), \r\n","                         lr = 0.05,\r\n","                         momentum=0.9,\r\n","                         weight_decay=1e-4)\r\n","\r\n","lmbda = lambda epoch: 0.98739\r\n","exp_lr_scheduler = optim.lr_scheduler.MultiplicativeLR(optimizer_ft, lr_lambda=lmbda)"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rhtIKV3aqv3w","executionInfo":{"status":"ok","timestamp":1615272417247,"user_tz":-540,"elapsed":1104330,"user":{"displayName":"Eunah choi","photoUrl":"","userId":"01586166681599241417"}},"outputId":"73fd0d09-9ad7-49ab-c320-7fd0f13e15b2"},"source":["model, best_idx, best_acc, train_loss, train_acc, valid_loss, valid_acc = train_model(model, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=50)"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Epoch 0/49\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["train Loss: 1.67 Acc: 55.2\n","valid Loss: 0.83 Acc: 70.7\n","==> best model saved - 0 / 70.7\n","Epoch 1/49\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["train Loss: 0.31 Acc: 90.2\n","valid Loss: 1.97 Acc: 65.7\n","Epoch 2/49\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["train Loss: 0.10 Acc: 96.7\n","valid Loss: 1.40 Acc: 77.4\n","==> best model saved - 2 / 77.4\n","Epoch 3/49\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["train Loss: 0.06 Acc: 98.1\n","valid Loss: 0.83 Acc: 84.1\n","==> best model saved - 3 / 84.1\n","Epoch 4/49\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["train Loss: 0.03 Acc: 99.3\n","valid Loss: 0.55 Acc: 86.2\n","==> best model saved - 4 / 86.2\n","Epoch 5/49\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["train Loss: 0.02 Acc: 99.6\n","valid Loss: 0.82 Acc: 86.2\n","Epoch 6/49\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["train Loss: 0.01 Acc: 99.9\n","valid Loss: 0.61 Acc: 87.4\n","==> best model saved - 6 / 87.4\n","Epoch 7/49\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["train Loss: 0.01 Acc: 99.9\n","valid Loss: 0.45 Acc: 88.3\n","==> best model saved - 7 / 88.3\n","Epoch 8/49\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["train Loss: 0.01 Acc: 99.9\n","valid Loss: 0.30 Acc: 92.1\n","==> best model saved - 8 / 92.1\n","Epoch 9/49\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["train Loss: 0.00 Acc: 99.9\n","valid Loss: 0.30 Acc: 92.5\n","==> best model saved - 9 / 92.5\n","Epoch 10/49\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["train Loss: 0.00 Acc: 100.0\n","valid Loss: 0.27 Acc: 91.6\n","Epoch 11/49\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["train Loss: 0.00 Acc: 99.9\n","valid Loss: 0.27 Acc: 92.1\n","Epoch 12/49\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["train Loss: 0.00 Acc: 100.0\n","valid Loss: 0.26 Acc: 92.1\n","Epoch 13/49\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["train Loss: 0.00 Acc: 100.0\n","valid Loss: 0.26 Acc: 91.6\n","Epoch 14/49\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["train Loss: 0.00 Acc: 100.0\n","valid Loss: 0.25 Acc: 92.1\n","Epoch 15/49\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["train Loss: 0.00 Acc: 100.0\n","valid Loss: 0.24 Acc: 92.5\n","Epoch 16/49\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["train Loss: 0.00 Acc: 100.0\n","valid Loss: 0.23 Acc: 92.9\n","==> best model saved - 16 / 92.9\n","Epoch 17/49\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["train Loss: 0.00 Acc: 100.0\n","valid Loss: 0.22 Acc: 92.9\n","Epoch 18/49\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["train Loss: 0.00 Acc: 100.0\n","valid Loss: 0.22 Acc: 92.9\n","Epoch 19/49\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["train Loss: 0.00 Acc: 100.0\n","valid Loss: 0.22 Acc: 93.3\n","==> best model saved - 19 / 93.3\n","Epoch 20/49\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["train Loss: 0.00 Acc: 100.0\n","valid Loss: 0.22 Acc: 93.3\n","Epoch 21/49\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["train Loss: 0.00 Acc: 100.0\n","valid Loss: 0.22 Acc: 93.3\n","Epoch 22/49\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["train Loss: 0.00 Acc: 100.0\n","valid Loss: 0.22 Acc: 93.7\n","==> best model saved - 22 / 93.7\n","Epoch 23/49\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["train Loss: 0.00 Acc: 100.0\n","valid Loss: 0.22 Acc: 93.7\n","Epoch 24/49\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["train Loss: 0.00 Acc: 100.0\n","valid Loss: 0.21 Acc: 93.3\n","Epoch 25/49\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["train Loss: 0.00 Acc: 99.9\n","valid Loss: 0.21 Acc: 93.7\n","Epoch 26/49\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["train Loss: 0.00 Acc: 99.9\n","valid Loss: 0.21 Acc: 93.3\n","Epoch 27/49\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["train Loss: 0.00 Acc: 100.0\n","valid Loss: 0.21 Acc: 92.9\n","Epoch 28/49\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["train Loss: 0.00 Acc: 100.0\n","valid Loss: 0.21 Acc: 92.9\n","Epoch 29/49\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["train Loss: 0.00 Acc: 100.0\n","valid Loss: 0.21 Acc: 93.7\n","Epoch 30/49\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["train Loss: 0.00 Acc: 100.0\n","valid Loss: 0.21 Acc: 93.3\n","Epoch 31/49\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["train Loss: 0.00 Acc: 100.0\n","valid Loss: 0.21 Acc: 93.7\n","Epoch 32/49\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["train Loss: 0.00 Acc: 100.0\n","valid Loss: 0.22 Acc: 93.7\n","Epoch 33/49\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["train Loss: 0.00 Acc: 100.0\n","valid Loss: 0.21 Acc: 93.7\n","Epoch 34/49\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["train Loss: 0.00 Acc: 100.0\n","valid Loss: 0.26 Acc: 92.1\n","Epoch 35/49\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["train Loss: 0.00 Acc: 100.0\n","valid Loss: 0.29 Acc: 91.6\n","Epoch 36/49\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["train Loss: 0.00 Acc: 99.9\n","valid Loss: 0.24 Acc: 92.5\n","Epoch 37/49\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["train Loss: 0.00 Acc: 99.9\n","valid Loss: 0.24 Acc: 92.9\n","Epoch 38/49\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["train Loss: 0.00 Acc: 99.9\n","valid Loss: 0.21 Acc: 93.7\n","Epoch 39/49\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["train Loss: 0.00 Acc: 100.0\n","valid Loss: 0.21 Acc: 94.1\n","==> best model saved - 39 / 94.1\n","Epoch 40/49\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["train Loss: 0.00 Acc: 100.0\n","valid Loss: 0.21 Acc: 94.1\n","Epoch 41/49\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["train Loss: 0.00 Acc: 100.0\n","valid Loss: 0.20 Acc: 94.1\n","Epoch 42/49\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["train Loss: 0.00 Acc: 100.0\n","valid Loss: 0.20 Acc: 93.7\n","Epoch 43/49\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["train Loss: 0.00 Acc: 100.0\n","valid Loss: 0.20 Acc: 93.7\n","Epoch 44/49\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["train Loss: 0.00 Acc: 99.9\n","valid Loss: 0.20 Acc: 93.7\n","Epoch 45/49\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["train Loss: 0.00 Acc: 100.0\n","valid Loss: 0.19 Acc: 93.7\n","Epoch 46/49\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["train Loss: 0.00 Acc: 100.0\n","valid Loss: 0.20 Acc: 93.7\n","Epoch 47/49\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["train Loss: 0.00 Acc: 100.0\n","valid Loss: 0.20 Acc: 93.7\n","Epoch 48/49\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["train Loss: 0.00 Acc: 100.0\n","valid Loss: 0.20 Acc: 94.1\n","Epoch 49/49\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"},{"output_type":"stream","text":["train Loss: 0.00 Acc: 100.0\n","valid Loss: 0.19 Acc: 93.7\n","Training complete in 18m 24s\n","Best valid Acc: 39 - 94.1\n","model saved\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":314},"id":"bEZ000ZHrD3m","executionInfo":{"status":"ok","timestamp":1615272421422,"user_tz":-540,"elapsed":958,"user":{"displayName":"Eunah choi","photoUrl":"","userId":"01586166681599241417"}},"outputId":"9890504e-9f4e-44b1-b43b-697bb7137475"},"source":["print('best model : %d - %1.f / %.1f'%(best_idx, valid_acc[best_idx], valid_loss[best_idx]))\r\n","fig, ax1 = plt.subplots()\r\n","\r\n","ax1.plot(train_acc, 'b-')\r\n","ax1.plot(valid_acc, 'r-')\r\n","plt.plot(best_idx, valid_acc[best_idx], 'ro')\r\n","ax1.set_xlabel('epoch')\r\n","# Make the y-axis label, ticks and tick labels match the line color.\r\n","ax1.set_ylabel('acc', color='k')\r\n","ax1.tick_params('y', colors='k')\r\n","\r\n","ax2 = ax1.twinx()\r\n","ax2.plot(train_loss, 'g-')\r\n","ax2.plot(valid_loss, 'k-')\r\n","plt.plot(best_idx, valid_loss[best_idx], 'ro')\r\n","ax2.set_ylabel('loss', color='k')\r\n","ax2.tick_params('y', colors='k')\r\n","\r\n","fig.tight_layout()\r\n","plt.show()"],"execution_count":14,"outputs":[{"output_type":"stream","text":["best model : 39 - 94 / 0.2\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 2 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"wfs7xpMlvUvN"},"source":[""],"execution_count":null,"outputs":[]}]}